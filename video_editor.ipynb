{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🎬 Automatic Video Cutter & Editor for Cooking Videos\n",
        "\n",
        "This notebook implements an **automatic video summarization and editing pipeline**. It is designed mainly for cooking videos, but the approach is generalizable.\n",
        "\n",
        "The system will:\n",
        "- Remove static (non-moving) sections from the videos.\n",
        "- Optionally use CLIP to select only those segments that are semantically similar to a provided text prompt.\n",
        "- Combine only segments with movement.\n",
        "- Allow you to choose between using just motion, just CLIP, or both.\n",
        "- Merge selected clips into a final video.\n",
        "- Limit the total output duration (e.g., 60 seconds for a reel).\n",
        "- Support both vertical and horizontal video formats, with an option to force a vertical (9:16) output.\n",
        "\n",
        "**Sources:**\n",
        "- OpenCV: [https://opencv.org/](https://opencv.org/)\n",
        "- MoviePy: [https://zulko.github.io/moviepy/](https://zulko.github.io/moviepy/)\n",
        "- CLIP (OpenAI): [https://github.com/openai/CLIP](https://github.com/openai/CLIP)\n",
        "- Whisper (OpenAI): [https://github.com/openai/whisper](https://github.com/openai/whisper)\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Install Dependencies\n",
        "\n",
        "Run the following cell to install the required libraries:\n"
      ],
      "metadata": {
        "id": "hCAPUuTYgyDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q opencv-python moviepy openai-whisper\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!apt-get -qq install -y ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcFChWGNh8ie",
        "outputId": "004c00aa-c10b-48cf-be6d-a55779c43196"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/800.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount Google Drive\n",
        "Make sure your videos are stored in a Drive folder. Run the cell below to mount your Drive:"
      ],
      "metadata": {
        "id": "aV9Iy0hFg4W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXdLyqdEgxuB",
        "outputId": "73b248e0-4a06-4713-b7ed-da8c03c8e338"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuration\n",
        "Customize these parameters as needed. You can adjust folder paths, thresholds, clip durations, and output format here."
      ],
      "metadata": {
        "id": "hLEvf5cwg8cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_folder = \"/content/drive/MyDrive/video_vongole\"\n",
        "output_folder = \"/content/drive/MyDrive/video_output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Text prompt (if empty, only motion will be used)\n",
        "text_prompt = \"chopping garlic\"  # e.g., \"chopping garlic\". Set to \"\" for no text filter.\n",
        "\n",
        "# Mode selection: use motion detection and/or CLIP similarity\n",
        "use_motion = True   # Use motion detection to remove static parts\n",
        "use_clip   = False   # Use CLIP to filter segments based on the text prompt\n",
        "\n",
        "# Clip duration parameters (in seconds)\n",
        "clip_min_duration = 0.3   # Minimum duration for an extracted clip\n",
        "clip_max_duration = 3.0   # Maximum duration for an extracted clip (used as default clip length)\n",
        "\n",
        "max_total_duration = 60.0  # Set to None for no limit\n",
        "\n",
        "force_vertical = False    #9:16 format\n",
        "# (width, height).\n",
        "output_resolution = None\n",
        "\n",
        "frame_sample_rate = 5      # seconds between frames to analyze\n",
        "movement_threshold = 400000  # number of pixel differences to consider a frame \"moving\"\n",
        "merge_clip_gap = 2.0\n",
        "clip_similarity_threshold = 0.35  # CLIP cosine similarity threshold"
      ],
      "metadata": {
        "id": "YpsPqMngg6qG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries and Load Models\n",
        "We import necessary libraries. CLIP is loaded for semantic similarity and OpenCV is used for motion detection. (Whisper is imported but not used in this version.)"
      ],
      "metadata": {
        "id": "1fiRhfEWhEtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if use_clip and text_prompt.strip() != \"\":\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "else:\n",
        "    clip_model, clip_preprocess = None, None\n",
        "\n",
        "# (Optional) Load Whisper model if needed in the future\n",
        "if False:\n",
        "    import whisper\n",
        "    whisper_model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "sLGHsbvxhJZo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Helper Functions\n",
        "Below we define functions to:\n",
        "\n",
        "Check motion between frames.\n",
        "\n",
        "Compute CLIP similarity between a frame and a text prompt.\n",
        "\n",
        "Merge segments that are close in time.\n",
        "\n",
        "Process individual videos and extract candidate segments."
      ],
      "metadata": {
        "id": "zWN-0llHhLZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_frame_similar_to_prompt(frame_img, prompt, threshold):\n",
        "    \"\"\"\n",
        "    Uses CLIP to determine if the given frame is semantically similar to the prompt.\n",
        "    Returns a tuple: (is_similar, similarity_score).\n",
        "    \"\"\"\n",
        "    # Preprocess and encode image\n",
        "    image = clip_preprocess(Image.fromarray(frame_img)).unsqueeze(0).to(device)\n",
        "    # Tokenize and encode text prompt\n",
        "    text_tokens = clip.tokenize([prompt]).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image)\n",
        "        text_features = clip_model.encode_text(text_tokens)\n",
        "        # Compute cosine similarity\n",
        "        similarities = (image_features @ text_features.T).squeeze(0)\n",
        "        best_score = similarities.max().item()\n",
        "    return best_score > threshold, best_score\n",
        "\n",
        "def detect_motion(prev_frame, frame):\n",
        "    \"\"\"\n",
        "    Compare two grayscale frames using absolute difference.\n",
        "    Returns True if the number of changed pixels exceeds the threshold.\n",
        "    \"\"\"\n",
        "    diff = cv2.absdiff(prev_frame, frame)\n",
        "    non_zero_count = np.count_nonzero(diff)\n",
        "    return non_zero_count > movement_threshold\n",
        "\n",
        "def merge_close_segments(segments, gap=2.0):\n",
        "    \"\"\"\n",
        "    Merge segments that are closer than `gap` seconds.\n",
        "    Each segment is a tuple: (start, end, score).\n",
        "    \"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "    segments = sorted(segments, key=lambda x: x[0])\n",
        "    merged = [segments[0]]\n",
        "    for start, end, score in segments[1:]:\n",
        "        last_start, last_end, last_score = merged[-1]\n",
        "        if start - last_end <= gap:\n",
        "            # Merge and update score as the maximum\n",
        "            merged[-1] = (last_start, max(end, last_end), max(score, last_score))\n",
        "        else:\n",
        "            merged.append((start, end, score))\n",
        "    return merged\n"
      ],
      "metadata": {
        "id": "eowfRdgPhMsk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Process Videos and Extract Candidate Segments\n",
        "This function processes a single video file:\n",
        "\n",
        "It samples frames at the specified rate.\n",
        "\n",
        "Checks for motion and (if enabled) CLIP similarity.\n",
        "\n",
        "Extracts candidate segments (each of fixed length defined by clip_max_duration).\n",
        "\n",
        "Merges segments that are too close in time."
      ],
      "metadata": {
        "id": "LHjwOisEhQAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, prompt):\n",
        "    \"\"\"\n",
        "    Processes a video file and returns a list of candidate segments.\n",
        "    Each candidate segment is a tuple: (video_path, start, end, score).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "\n",
        "    print(f\"Processing {video_path} | Duration: {duration:.2f}s\")\n",
        "\n",
        "    candidate_segments = []\n",
        "    prev_frame_gray = None\n",
        "    # Use clip_max_duration as the duration of each extracted segment\n",
        "    clip_duration = clip_max_duration\n",
        "\n",
        "    t = 0\n",
        "    while t < duration:\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            t += frame_sample_rate\n",
        "            continue\n",
        "\n",
        "        # Convert to grayscale for motion detection\n",
        "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        motion = True\n",
        "        if use_motion and prev_frame_gray is not None:\n",
        "            motion = detect_motion(prev_frame_gray, frame_gray)\n",
        "        prev_frame_gray = frame_gray\n",
        "\n",
        "        # Initialize similarity check result and score\n",
        "        similar = True\n",
        "        similarity_score = 1.0  # default score if CLIP is not used\n",
        "        if use_clip and text_prompt.strip() != \"\":\n",
        "            similar, similarity_score = is_frame_similar_to_prompt(frame, text_prompt, clip_similarity_threshold)\n",
        "\n",
        "        # Accept the frame if conditions are met (must be moving and, if prompt is used, semantically similar)\n",
        "        if motion and similar:\n",
        "            start = max(0, t - clip_duration/2)\n",
        "            end = min(duration, t + clip_duration/2)\n",
        "            # Ensure the segment is at least clip_min_duration long\n",
        "            if (end - start) >= clip_min_duration:\n",
        "                candidate_segments.append((start, end, similarity_score))\n",
        "        t += frame_sample_rate\n",
        "    cap.release()\n",
        "\n",
        "    # Merge segments that are close in time\n",
        "    merged_segments = merge_close_segments(candidate_segments, gap=merge_clip_gap)\n",
        "    return video_path, merged_segments"
      ],
      "metadata": {
        "id": "AS1Y8E_yhOVe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Process All Videos and Select Best Segments\n",
        "This section loops over all video files in the input folder, processes them, and then selects the best segments if the total duration exceeds the maximum allowed. If the maximum duration is exceeded, the segments are sorted by significance (CLIP score) and the best ones are selected."
      ],
      "metadata": {
        "id": "v4KUA9wthUfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def get_all_video_files(folder):\n",
        "    video_extensions = [\"*.mp4\", \"*.MOV\", \"*.avi\", \"*.mkv\"]\n",
        "    files = []\n",
        "    for ext in video_extensions:\n",
        "        files.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return files\n",
        "\n",
        "# Process videos and collect candidate segments\n",
        "all_candidate_segments = []  # List of tuples: (video_path, start, end, score)\n",
        "video_files = get_all_video_files(input_folder)\n",
        "print(f\"Found {len(video_files)} video(s).\")\n",
        "\n",
        "for video_file in video_files:\n",
        "    vpath, segments = process_video(video_file, text_prompt)\n",
        "    for seg in segments:\n",
        "        start, end, score = seg\n",
        "        all_candidate_segments.append((vpath, start, end, score))\n",
        "\n",
        "print(f\"Total candidate segments found: {len(all_candidate_segments)}\")\n",
        "\n",
        "# If a maximum total duration is set, select the best segments based on score.\n",
        "def select_segments(segments, max_duration):\n",
        "    if max_duration is None:\n",
        "        return segments\n",
        "    # Sort segments by descending score\n",
        "    segments_sorted = sorted(segments, key=lambda x: x[3], reverse=True)\n",
        "    selected = []\n",
        "    cumulative_duration = 0.0\n",
        "    for seg in segments_sorted:\n",
        "        vpath, start, end, score = seg\n",
        "        seg_duration = end - start\n",
        "        if cumulative_duration + seg_duration <= max_duration:\n",
        "            selected.append(seg)\n",
        "            cumulative_duration += seg_duration\n",
        "        if cumulative_duration >= max_duration:\n",
        "            break\n",
        "    # For a coherent montage, sort the selected segments by video path and start time\n",
        "    selected = sorted(selected, key=lambda x: (x[0], x[1]))\n",
        "    return selected\n",
        "\n",
        "selected_segments = select_segments(all_candidate_segments, max_total_duration)\n",
        "print(f\"Selected segments after duration filtering: {len(selected_segments)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXaR0hdhVs2",
        "outputId": "7adb1604-8065-44cf-ed24-07ea7b12b8cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 video(s).\n",
            "Processing /content/drive/MyDrive/video_vongole/IMG_6134.MOV | Duration: 27.70s\n",
            "Processing /content/drive/MyDrive/video_vongole/IMG_6135.MOV | Duration: 5.66s\n",
            "Processing /content/drive/MyDrive/video_vongole/IMG_6136.MOV | Duration: 37.87s\n",
            "Processing /content/drive/MyDrive/video_vongole/IMG_6139.MOV | Duration: 5.29s\n",
            "Processing /content/drive/MyDrive/video_vongole/IMG_6140.MOV | Duration: 6.25s\n",
            "Total candidate segments found: 5\n",
            "Selected segments after duration filtering: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Crop (Force) to Vertical Format (Optional)\n",
        "If you want to force the output video to vertical (9:16), the following helper function will crop each clip to a vertical format."
      ],
      "metadata": {
        "id": "0S_21lIfhXfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def force_vertical_format(clip):\n",
        "    \"\"\"\n",
        "    Crops the clip to a vertical 9:16 aspect ratio from the center.\n",
        "    \"\"\"\n",
        "    w, h = clip.size\n",
        "    target_ratio = 9/16\n",
        "    target_width = int(h * target_ratio)\n",
        "    if w > target_width:\n",
        "        # Crop horizontally: center crop\n",
        "        x1 = (w - target_width) // 2\n",
        "        x2 = x1 + target_width\n",
        "        clip = clip.crop(x1=x1, x2=x2)\n",
        "    else:\n",
        "        # If the clip is too narrow, resize while maintaining aspect ratio\n",
        "        clip = clip.resize(width=target_width)\n",
        "    return clip"
      ],
      "metadata": {
        "id": "0al1k5EFhaH6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract and Concatenate Clips into Final Video\n",
        "Now we extract the subclips from each selected segment and concatenate them using MoviePy. We also apply the vertical format if forced, and finally write the output video."
      ],
      "metadata": {
        "id": "aa3yW0ijhdcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_clips = []\n",
        "for seg in selected_segments:\n",
        "    video_path, start, end, score = seg\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {video_path}: {e}\")\n",
        "        continue\n",
        "    clip = video.subclip(start, end)\n",
        "    # Resize to output resolution if specified\n",
        "    if output_resolution is not None:\n",
        "        clip = clip.resize(newsize=output_resolution)\n",
        "    # Force vertical format if required\n",
        "    if force_vertical:\n",
        "        clip = force_vertical_format(clip)\n",
        "    final_clips.append(clip)\n",
        "\n",
        "if final_clips:\n",
        "    final_video = concatenate_videoclips(final_clips, method=\"compose\")\n",
        "    # If the final video is longer than max_total_duration, trim it.\n",
        "    if max_total_duration is not None and final_video.duration > max_total_duration:\n",
        "        final_video = final_video.subclip(0, max_total_duration)\n",
        "    output_path = os.path.join(output_folder, \"final_output.mp4\")\n",
        "    print(f\"Writing final video to {output_path} (duration: {final_video.duration:.2f}s)\")\n",
        "    final_video.write_videofile(output_path, codec=\"libx264\", audio=True)\n",
        "else:\n",
        "    print(\"⚠️ No valid segments found for the final montage.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHY_GTppheqB",
        "outputId": "b34563b7-707d-46fd-ed7b-d451bc0a5e6a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing final video to /content/drive/MyDrive/video_output/final_output.mp4 (duration: 43.70s)\n",
            "Moviepy - Building video /content/drive/MyDrive/video_output/final_output.mp4.\n",
            "MoviePy - Writing audio in final_outputTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video /content/drive/MyDrive/video_output/final_output.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|██████████| 2620/2620 [11:07<00:00,  2.96it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/video_vongole/IMG_6140.MOV, 6220800 bytes wanted but 0 bytes read,at frame 374/375, at time 6.24/6.25 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/video_output/final_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Future Extensions\n",
        "* Whisper Integration: Use Whisper for audio transcription to filter or annotate clips based on speech.\n",
        "\n",
        "* Additional Visual Filters: Integrate object detection, pose\n",
        "estimation (e.g., with MediaPipe) or action recognition (using I3D, SlowFast, or Swin Transformer).\n",
        "\n",
        "* Different Output Formats: Enable export to various social media formats (vertical 9:16 for TikTok/Reels, horizontal for YouTube, etc.).\n",
        "\n",
        "* GUI Integration: Build an interactive interface with Gradio or Streamlit.\n",
        "\n"
      ],
      "metadata": {
        "id": "VXZbCLFrhjep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sources\n",
        "OpenCV\n",
        "\n",
        "MoviePy\n",
        "\n",
        "CLIP (OpenAI)\n",
        "\n",
        "Whisper (OpenAI)\n",
        "\n"
      ],
      "metadata": {
        "id": "SiRFj7NjhwY7"
      }
    }
  ]
}